# agent/sql_agent.py
"""
SQL Agent - Evidence-Driven DBA AI System
==========================================

This module integrates:
- DecisionEngine: DBA-style reasoning gates
- DynamicSQLGenerator: Runtime SQL generation
- Signal normalization from RCA output

CRITICAL PRINCIPLES:
- Recommendations are context-aware
- Irrelevant fixes are suppressed
- SQL recommendations feel reasoned, not templated
- DBAs can trust and execute the output

CRITICAL RULES (DO NOT VIOLATE):
- âŒ DO NOT store SQL queries as constants
- âŒ DO NOT reuse the same SQL format everywhere  
- âŒ DO NOT generate SQL without consulting decision category + signals
- âŒ DO NOT show all tuning options for every SQL
- âœ… SQL must be generated at runtime
- âœ… SQL text must change when signals change
- âœ… Same SQL_ID with different workload â†’ different SQL text
- âœ… Explain why something is shown AND why others are hidden
"""

from typing import Any, Dict, List, Optional
import pandas as pd
from pandas import DataFrame

from engine.sql_intelligence_engine import SQLIntelligenceEngine

# Import Decision Engine and Dynamic SQL Generator
from engine.decision_engine import (
    ActionType, ActionType, ActionType, DecisionEngine, DecisionResult, SQLCategory, NormalizedSignals, SignalNormalizer
)
from engine.dynamic_sql_generator import DynamicSQLGenerator, GeneratedSQL, DBAActionPlan


class SQLAgent:
    """
    SQL Agent - Evidence-Driven DBA AI System
    
    Integrates Decision Engine (what is allowed) with Dynamic SQL Generator (how SQL is written).
    Every recommendation passes through decision gates before generation.
    """
    
    def __init__(self, rca_result: dict = None) -> None:
        self.data = rca_result or {}
        
        # Initialize the DBA reasoning layer
        self.decision_engine = DecisionEngine()
        self.sql_generator = DynamicSQLGenerator()
        self.signal_normalizer = SignalNormalizer()

    def explain(self):
        insights = []

        peak = self.data.get("summary", {}).get("detected_peak")
        if peak:
            # Handle different peak data structures
            if isinstance(peak, dict):
                # Try different possible fields for time information
                peak_time = peak.get('peak_time') or peak.get('start') or peak.get('reason') or 'Unknown time'
                active_sessions = peak.get('active_sessions') or peak.get('total_sessions') or 'Unknown'
                
                # Build a descriptive message based on available data
                if 'start' in peak and 'end' in peak:
                    try:
                        start_str = peak['start'].strftime('%H:%M:%S') if hasattr(peak['start'], 'strftime') else str(peak['start'])
                        end_str = peak['end'].strftime('%H:%M:%S') if hasattr(peak['end'], 'strftime') else str(peak['end'])
                        insights.append(f"High load period: {start_str} - {end_str} with {active_sessions} sessions")
                    except:
                        insights.append(f"High load detected: {peak.get('reason', 'Database activity spike')}")
                else:
                    insights.append(f"Peak load detected with ~{active_sessions} active sessions")
            else:
                insights.append("High load period detected")

        for sql in self.data.get("top_sql", []):
            insights.append(
                f"SQL {sql['sql_id']} | elapsed={sql['elapsed']}s | "
                f"cpu={sql['cpu']} | risk={sql['risk']}"
            )

        for w in self.data.get("top_wait_events", []):
            if w["pct_of_db_time"] > 40:
                insights.append(
                    f"Major DB time contributor: {w['statistic_name']} "
                    f"({w['pct_of_db_time']}%)"
                )

        if not insights:
            insights.append("No critical bottleneck detected")

        return insights

    async def generate_fix_recommendations(self, sql_id: str, query_data: dict) -> dict:
        """Generate comprehensive fix recommendations with STRICT FIXES only for zero values and risk logic"""
        
        try:
            # Load AWR/CSV data
            csv_data = self._load_csv_data()  # type: Dict[str, Any]
            
            # FIX 1ï¸âƒ£: Extract ACTUAL values from AWR data (never show zero incorrectly)
            elapsed_time = float(query_data.get('elapsed', 0) or query_data.get('elapsed_time', 0))
            cpu_time = float(query_data.get('cpu', 0) or query_data.get('cpu_time', 0))
            executions = int(query_data.get('executions', 0))
            elapsed_per_exec = float(query_data.get('elapsed_per_exec', 0))
            pctcpu = float(query_data.get('pctcpu', 0))
            pctio = float(query_data.get('pctio', 0))
            pcttotal = float(query_data.get('pcttotal', 0))
            
            # CRITICAL: If values are zero but we have executions, try to get from CSV
            if (elapsed_time == 0 or cpu_time == 0 or pctio == 0) and executions > 0 and csv_data.get('sql_stats') is not None:
                sql_stats = csv_data.get('sql_stats')
                if sql_stats is not None:
                    # Find this SQL_ID in CSV data
                    sql_stats.columns = sql_stats.columns.str.strip().str.lower()
                    sql_row: pd.DataFrame = sql_stats[sql_stats['sql_id'] == sql_id]
                    if not sql_row.empty:
                        # Get actual values from CSV
                        elapsed_col = self._find_column(sql_stats, ["elapsed__time_s", "elapsed_time_s", "elapsed"])
                        cpu_col = self._find_column(sql_stats, ["pctcpu"])
                        io_col = self._find_column(sql_stats, ["pctio"])
                        if elapsed_col and elapsed_time == 0:
                            csv_elapsed = pd.to_numeric(sql_row[elapsed_col].iloc[0], errors="coerce")
                            if csv_elapsed and csv_elapsed > 0:
                                elapsed_time = float(csv_elapsed)
                                elapsed_per_exec: float = elapsed_time / max(executions, 1)
                        if cpu_col and pctcpu == 0:
                            csv_pctcpu = pd.to_numeric(sql_row[cpu_col].iloc[0], errors="coerce")
                            if csv_pctcpu and csv_pctcpu > 0:
                                pctcpu = float(csv_pctcpu)
                        if io_col:
                            csv_pctio = pd.to_numeric(sql_row[io_col].iloc[0], errors="coerce")
                            if csv_pctio is not None and csv_pctio >= 0:  # Accept 0 as valid
                                pctio = float(csv_pctio)
            
            # Get SQL text for RMAN detection
            sql_text: str = self._get_sql_text_for_query(sql_id, csv_data) if csv_data.get('sql_stats') is not None else ""
            sql_module: str = str(query_data.get('sql_module', '')).lower()
            
            # FIX 2ï¸âƒ£: STRICT Risk Classification Logic
            if elapsed_time > 50 or pctcpu > 90 or pcttotal > 30:
                risk_level = "HIGH"
            elif elapsed_time < 50 and executions > 500:
                risk_level = "MEDIUM"  
            elif elapsed_time < 5 and cpu_time < 5 and executions < 500:
                risk_level = "LOW"
            else:
                risk_level = "MEDIUM"  # Default for edge cases
            
            # Update query_data with fixed values
            query_data_fixed = query_data.copy()
            query_data_fixed.update({
                'elapsed': elapsed_time,
                'cpu': cpu_time,
                'elapsed_per_exec': elapsed_per_exec,
                'pctcpu': pctcpu,
                'pctio': pctio,
                'risk': risk_level
            })
            
            # ================================================================
            # NEW: Evidence-Driven DBA AI System
            # ================================================================
            # Use Decision Engine + Dynamic SQL Generator for evidence-based recommendations
            return await self._generate_evidence_driven_recommendations(sql_id, query_data_fixed, csv_data)
            
        except Exception as e:
            print(f"Error in generate_fix_recommendations: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_legacy_recommendations(sql_id, query_data, {})

    def _find_column(self, df, candidates):
        """Find column by trying multiple name candidates"""
        for col in candidates:
            if col in df.columns:
                return col
        return None
    
    # =========================================================================
    # EVIDENCE-DRIVEN DBA AI SYSTEM
    # =========================================================================
    
    async def _generate_evidence_driven_recommendations(self, sql_id: str, 
                                                         query_data: dict, 
                                                         csv_data: dict) -> dict:
        """
        Generate evidence-driven recommendations using Decision Engine + Dynamic SQL Generator.
        
        This is the core of the Senior DBA AI system:
        1. Normalize signals from RCA output
        2. Pass through decision gates (DBA reasoning layer)
        3. Generate targeted fixes based on allowed actions
        4. Generate DBA Action Plan with timeframe-based actions
        5. Include full explainability (why shown, why hidden)
        
        CRITICAL: Same SQL_ID with different signals MUST produce different output.
        """
        try:
            # Step 1: Normalize signals into standardized format
            signals: NormalizedSignals = self._normalize_signals_from_query_data(sql_id, query_data, csv_data)
            
            # Step 2: Evaluate through Decision Engine (DBA reasoning layer)
            decision: DecisionResult = self.decision_engine.evaluate(signals)
            
            # Step 3: Generate dynamic SQL commands based on decision
            generated_commands: List[GeneratedSQL] = self.sql_generator.generate_all(decision)
            
            # Step 4: Generate DBA Action Plan (timeframe-based actions)
            action_plan: DBAActionPlan = self.sql_generator.generate_action_plan(decision)
            
            # Step 5: Format for UI (maintaining existing contract)
            return self._format_evidence_driven_output(
                sql_id, query_data, decision, generated_commands, signals, action_plan
            )
            
        except Exception as e:
            print(f"Error in evidence-driven recommendations: {e}")
            import traceback
            traceback.print_exc()
            # Fallback generates minimal output without templated SQL
            return self._generate_minimal_fallback(sql_id, query_data, str(e))
    
    def _normalize_signals_from_query_data(self, sql_id: str, 
                                            query_data: dict,
                                            csv_data: dict) -> NormalizedSignals:
        """
        Transform query_data into normalized signals.
        This is the ONLY input to decision logic.
        """
        # Extract core metrics
        elapsed_time = float(query_data.get('elapsed', 0) or query_data.get('elapsed_time', 0))
        cpu_time = float(query_data.get('cpu', 0) or query_data.get('cpu_time', 0))
        executions = int(query_data.get('executions', 0) or 0)
        elapsed_per_exec = float(query_data.get('elapsed_per_exec', 0) or 0)
        
        # Calculate average execution time if not provided
        if elapsed_per_exec == 0 and executions > 0 and elapsed_time > 0:
            elapsed_per_exec: float = elapsed_time / executions
        
        # Get percentages
        pctcpu = float(query_data.get('pctcpu', 0) or 0)
        pctio = float(query_data.get('pctio', 0) or 0)
        pcttotal = float(query_data.get('pcttotal', 0) or query_data.get('db_time_pct', 0) or 0)
        
        # If pctio not available, estimate from elapsed vs cpu
        if pctio == 0 and elapsed_time > 0 and cpu_time >= 0:
            non_cpu_time: float = max(0, elapsed_time - cpu_time)
            pctio: float | int = (non_cpu_time / elapsed_time) * 100 if elapsed_time > 0 else 0
        
        # If pctcpu not available, estimate from cpu vs elapsed
        if pctcpu == 0 and elapsed_time > 0 and cpu_time > 0:
            pctcpu: float = (cpu_time / elapsed_time) * 100
        
        # Get optional context
        sql_text = query_data.get('sql_text', None)
        if sql_text is None and csv_data.get('sql_stats') is not None:
            sql_text: str = self._get_sql_text_for_query(sql_id, csv_data)
        
        sql_module = query_data.get('sql_module', None)
        
        return NormalizedSignals(
            sql_id=sql_id,
            executions=executions,
            total_elapsed=elapsed_time,
            avg_exec_time=elapsed_per_exec,
            cpu_time=cpu_time,
            cpu_pct=pctcpu,
            io_wait_pct=pctio,
            db_time_pct=pcttotal,
            sql_text=sql_text,
            sql_module=sql_module
        )
    
    def _format_evidence_driven_output(self, sql_id: str, 
                                        query_data: dict,
                                        decision: DecisionResult,
                                        generated_commands: List[GeneratedSQL],
                                        signals: NormalizedSignals,
                                        action_plan: Optional[DBAActionPlan] = None) -> dict:
        """
        Format the evidence-driven output to match existing UI contract.
        
        Includes:
        - recommended_indexes
        - query_rewrite
        - risk_assessment
        - risk_level
        - exact_commands
        - dba_action_plan (timeframe-based actions)
        - why_shown / why_hidden (explainability)
        """
        # Build recommended_indexes from generated commands
        index_recommendations = []
        for cmd in generated_commands:
            if cmd.action in ['INDEX_REVIEW', 'INDEX_CREATION', 'ACCESS_PATH_OPTIMIZATION']:
                index_recommendations.append(f"â€¢ {cmd.intent}")
                index_recommendations.append(f"  Reason: {cmd.explanation}")
        
        if not index_recommendations:
            if decision.category == SQLCategory.CHATTY_SQL:
                index_recommendations = ["â€¢ Index changes not recommended for chatty SQL pattern",
                                         "â€¢ Focus on application-level optimization instead"]
            elif decision.category == SQLCategory.LOW_PRIORITY:
                index_recommendations = ["â€¢ No index changes required",
                                         "â€¢ Current access patterns are efficient"]
            else:
                index_recommendations = ["â€¢ Index analysis included in execution plan review"]
        
        # Build query_rewrite recommendations
        rewrite_recommendations = []
        for cmd in generated_commands:
            if cmd.action in ['SQL_REWRITE', 'APPLICATION_THROTTLING', 'RESULT_CACHING', 'BIND_TUNING']:
                rewrite_recommendations.append(f"â€¢ {cmd.intent}")
                rewrite_recommendations.append(f"  Reason: {cmd.explanation}")
        
        if not rewrite_recommendations:
            if decision.category == SQLCategory.IO_BOUND_SQL:
                rewrite_recommendations = ["â€¢ Query structure is acceptable",
                                           "â€¢ Focus on access path optimization for IO reduction"]
            elif decision.category == SQLCategory.LOW_PRIORITY:
                rewrite_recommendations = ["â€¢ No query rewrite required",
                                           "â€¢ Query performance is within acceptable range"]
            else:
                rewrite_recommendations = ["â€¢ See execution plan analysis for optimization opportunities"]
        
        # Build risk assessment with EXPLAINABILITY
        risk_level: str = self._map_category_to_risk_level(decision.category, signals)
        risk_assessment: str = self._build_risk_assessment(decision, signals)
        
        # Build exact commands from generated SQL
        exact_commands: str = self._format_exact_commands(decision, generated_commands, signals)
        
        # Build DBA Action Plan (timeframe-based)
        action_plan_text: str = ""
        action_plan_data = {}
        if action_plan:
            action_plan_text: str = action_plan.to_formatted_string()
            action_plan_data: Dict[str, Any] = action_plan.to_dict()
        
        return {
            "recommended_indexes": "\n".join(index_recommendations),
            "query_rewrite": "\n".join(rewrite_recommendations),
            "risk_assessment": risk_assessment,
            "risk_level": risk_level,
            "exact_commands": exact_commands,
            
            # DBA ACTION PLAN (MANDATORY per spec - timeframe based)
            "dba_action_plan": action_plan_text,
            "action_plan_data": action_plan_data,
            
            # EXPLAINABILITY (MANDATORY per spec)
            "why_shown": decision.why_shown,
            "why_hidden": decision.why_hidden,
            
            # Decision metadata
            "decision_category": decision.category.value,
            "allowed_actions": [a.value for a in decision.allowed_actions],
            "blocked_actions": [a.value for a in decision.blocked_actions],
            "reasoning": decision.reasoning,
            
            # Signal fingerprint (proves dynamic generation)
            "signal_fingerprint": f"exec={signals.executions}|cpu={signals.cpu_pct:.1f}|io={signals.io_wait_pct:.1f}|avg={signals.avg_exec_time:.4f}"
        }
    
    def _map_category_to_risk_level(self, category: SQLCategory, 
                                     signals: NormalizedSignals) -> str:
        """Map decision category + signals to risk level"""
        # High risk categories
        if category in [SQLCategory.BATCH_SQL, SQLCategory.IO_BOUND_SQL]:
            if signals.total_elapsed > 50 or signals.io_wait_pct > 90:
                return "HIGH"
            elif signals.total_elapsed > 20 or signals.io_wait_pct > 70:
                return "MEDIUM"
        
        if category == SQLCategory.CPU_BOUND_SQL:
            if signals.cpu_pct > 90 or signals.cpu_time > 50:
                return "HIGH"
            return "MEDIUM"
        
        if category == SQLCategory.CHATTY_SQL:
            if signals.executions > 5000:
                return "HIGH"
            return "MEDIUM"
        
        if category == SQLCategory.MIXED_PROFILE_SQL:
            return "MEDIUM"
        
        # Low priority
        return "LOW"
    
    def _build_risk_assessment(self, decision: DecisionResult, 
                                signals: NormalizedSignals) -> str:
        """Build detailed risk assessment with explainability"""
        lines = []
        
        # Category header
        category_descriptions: Dict[SQLCategory, str] = {
            SQLCategory.BATCH_SQL: "BATCH/REPORT SQL - Slow per execution, low frequency",
            SQLCategory.CHATTY_SQL: "CHATTY/OLTP SQL - Fast but excessive frequency", 
            SQLCategory.IO_BOUND_SQL: "IO-BOUND SQL - High IO wait indicates inefficient data access",
            SQLCategory.CPU_BOUND_SQL: "CPU-BOUND SQL - High CPU with efficient IO",
            SQLCategory.MIXED_PROFILE_SQL: "MIXED PROFILE SQL - Multiple concerning characteristics",
            SQLCategory.LOW_PRIORITY: "LOW PRIORITY SQL - No tuning justified"
        }
        
        lines.append(f"ðŸ“Š **{category_descriptions.get(decision.category, decision.category.value)}**")
        lines.append("")
        
        # Signal summary
        lines.append(f"**Signal Analysis for SQL_ID {signals.sql_id}:**")
        for signal in decision.why_shown:
            lines.append(f"  â€¢ {signal}")
        lines.append("")
        
        # Reasoning
        lines.append("**DBA Reasoning:**")
        for reason in decision.reasoning:
            lines.append(f"  â†’ {reason}")
        lines.append("")
        
        # What's allowed
        lines.append("**Allowed Actions:**")
        for action in decision.allowed_actions:
            lines.append(f"  âœ… {action.value}")
        lines.append("")
        
        # What's blocked (explainability)
        if decision.why_hidden:
            lines.append("**Suppressed Actions (and why):**")
            for hidden in decision.why_hidden:
                lines.append(f"  âŒ {hidden}")
        
        return "\n".join(lines)
    
    def _format_exact_commands(self, decision: DecisionResult,
                                generated_commands: List[GeneratedSQL],
                                signals: NormalizedSignals) -> str:
        """
        Format generated SQL commands for DBA execution.
        
        CRITICAL: Each command includes signal context and fingerprint
        to prove dynamic generation.
        """
        if not generated_commands:
            return f"""ðŸ“‹ SQL_ID {signals.sql_id}: No actionable commands generated
Category: {decision.category.value}
Status: Continue monitoring

Why no commands were generated:
{chr(10).join('  â€¢ ' + reason for reason in decision.why_hidden)}"""
        
        sections = []
        
        # Header with signal fingerprint (PROOF of dynamic generation)
        fingerprint: str = f"exec={signals.executions}|cpu={signals.cpu_pct:.1f}%|io={signals.io_wait_pct:.1f}%|avg={signals.avg_exec_time:.4f}s"
        
        sections.append(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        sections.append(f"ðŸ”§ **DBA Commands for SQL_ID {signals.sql_id}**")
        sections.append(f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        sections.append(f"Category: {decision.category.value}")
        sections.append(f"Signal Fingerprint: {fingerprint}")
        sections.append(f"Commands Generated: {len(generated_commands)}")
        sections.append("")
        sections.append("ðŸ“Š **SIGNAL CONTEXT** (same SQL + different signals = different commands)")
        sections.append(f"  â€¢ IO Wait: {signals.io_wait_pct:.1f}% â†’ {'IOSTATS+PARALLEL' if signals.io_wait_pct > 70 else 'IOSTATS' if signals.io_wait_pct > 40 else 'minimal IO focus'}")
        sections.append(f"  â€¢ CPU: {signals.cpu_pct:.1f}% â†’ {'COST+PREDICATE+PROJECTION' if signals.cpu_pct > 90 else 'COST+PREDICATE' if signals.cpu_pct > 70 else 'COST' if signals.cpu_pct > 40 else 'minimal CPU focus'}")
        sections.append(f"  â€¢ Executions: {signals.executions:,} â†’ {'PEEKED_BINDS+ADAPTIVE' if signals.executions > 1000 else 'OUTLINE+PARALLEL' if signals.executions < 50 else 'standard analysis'}")
        sections.append(f"  â€¢ Avg Exec: {signals.avg_exec_time:.4f}s â†’ {'batch/report pattern' if signals.avg_exec_time > 5 else 'OLTP pattern' if signals.avg_exec_time < 0.1 else 'moderate'}")
        sections.append("")
        sections.append("-" * 65)
        sections.append("")
        
        # Group commands by action type
        for i, cmd in enumerate(generated_commands, 1):
            sections.append(f"**[{i}] {cmd.action}** ({cmd.category})")
            sections.append(f"ðŸ“Œ Intent: {cmd.intent}")
            sections.append(f"ðŸ“ Explanation: {cmd.explanation}")
            sections.append("")
            sections.append("```sql")
            sections.append(cmd.sql)
            sections.append("```")
            sections.append("")
        
        # Footer with validation info
        sections.append("-" * 65)
        sections.append("âš ï¸ **IMPORTANT**: Review commands before execution in production")
        sections.append("These commands are generated based on RCA signals and should be validated by DBA")
        sections.append("")
        sections.append("ðŸ” **PROOF OF DYNAMIC GENERATION**:")
        sections.append(f"  â€¢ Signal Fingerprint: {fingerprint}")
        sections.append(f"  â€¢ Different signals would produce different SQL text")
        sections.append(f"  â€¢ Format options assembled at runtime, not from templates")
        
        return "\n".join(sections)

    def _detect_rman_maintenance_strict(self, sql_module: str, sql_text: str) -> bool:
        """STRICT RMAN/Maintenance detection using explicit criteria"""
        
        # Check sql_module for maintenance indicators
        rman_module_keywords = ['rman', 'backup', 'maintenance', 'dbms_backup', 'ksxm']  # type: List[str]
        module_match = any(keyword in sql_module for keyword in rman_module_keywords)  # type: bool
        
        # Check sql_text for system operations  
        if sql_text:
            sql_text_upper = sql_text.upper()  # type: str
            rman_text_keywords = [  # type: List[str]
                'DBMS_BACKUP_RESTORE', 'X$K', 'KSXM:TAKE_SNPSHOT', 
                'SYS.DBMS_BACKUP', 'BACKUP', 'RESTORE'
            ]
            text_match: bool = any(keyword in sql_text_upper for keyword in rman_text_keywords)
            return module_match or text_match
        
        return module_match

    def _calculate_strict_risk_level(self, elapsed_time: float, cpu_time: float, executions: int, pctcpu: float, pcttotal: float) -> dict:
        """STRICT RISK ENGINE LOGIC - realistic and data-based"""
        
        risk_score = 0
        risk_factors = []
        
        # HIGH RISK criteria
        if elapsed_time > 50:
            risk_score += 40
            risk_factors.append(f"Very high elapsed time: {elapsed_time:.1f}s")
        elif elapsed_time > 20:
            risk_score += 25
            risk_factors.append(f"High elapsed time: {elapsed_time:.1f}s")
        
        if pctcpu > 90:
            risk_score += 35
            risk_factors.append(f"Critical CPU consumption: {pctcpu:.1f}%")
        elif pctcpu > 70:
            risk_score += 20
            risk_factors.append(f"High CPU consumption: {pctcpu:.1f}%")
        
        if pcttotal > 30:
            risk_score += 30
            risk_factors.append(f"High DB time percentage: {pcttotal:.1f}%")
        elif pcttotal > 15:
            risk_score += 15
            risk_factors.append(f"Significant DB time usage: {pcttotal:.1f}%")
        
        # MEDIUM RISK criteria - High executions even with low elapsed
        if executions > 1000:
            risk_score += 25
            risk_factors.append(f"Very high execution frequency: {executions:,}")
        elif executions > 500:
            risk_score += 15
            risk_factors.append(f"High execution frequency: {executions:,}")
        
        # Determine final risk level
        if risk_score >= 60:
            level = "CRITICAL"
        elif risk_score >= 40:
            level = "HIGH"  
        elif risk_score >= 20 or executions > 500:  # STRICT RULE: High exec = MEDIUM minimum
            level = "MEDIUM"
        else:
            level = "LOW"
        
        return {
            'level': level,
            'score': risk_score,
            'factors': risk_factors,
            'elapsed_time': elapsed_time,
            'cpu_time': cpu_time,
            'executions': executions,
            'pctcpu': pctcpu,
            'pcttotal': pcttotal
        }

    def _generate_controlled_intelligent_recommendations(self, sql_id: str, query_data: dict, csv_data: dict, sql_text: str) -> dict:
        """Generate detailed professional recommendations ONLY for high-confidence scenarios"""
        
        try:
            elapsed_time = float(query_data.get('elapsed', 0))
            cpu_time = float(query_data.get('cpu', 0))
            executions = int(query_data.get('executions', 0))
            elapsed_per_exec = float(query_data.get('elapsed_per_exec', 0))
            
            # Determine the specific high-confidence scenario
            
            # SCENARIO 1: Confirmed RMAN operation
            if (any(keyword in sql_text.upper() for keyword in ['DBMS_BACKUP_RESTORE', 'X$K', 'KSXM:TAKE_SNPSHOT']) or
                'RMAN@' in str(query_data.get('sql_module', ''))):
                
                return {
                    "recommended_indexes": f"ðŸ”§ RMAN/Maintenance Operation Analysis\nâ€¢ SQL_ID {sql_id}: Background maintenance operation detected\nâ€¢ Index optimization not applicable during backup operations\nâ€¢ Focus on scheduling rather than query optimization\nâ€¢ Monitor impact on concurrent OLTP workload",
                    
                    "query_rewrite": f"ðŸ”§ RMAN/Maintenance Query Analysis\nâ€¢ SQL_ID {sql_id}: System-generated maintenance SQL\nâ€¢ Query rewrite not recommended for backup operations\nâ€¢ Consider backup parallelism adjustment if performance issues persist\nâ€¢ Review RMAN configuration: CONFIGURE DEVICE TYPE DISK PARALLELISM",
                    
                    "risk_assessment": f"ðŸš¨ CRITICAL: SQL_ID {sql_id} - Confirmed RMAN/Maintenance Operation\n\nðŸ“Š Performance Impact Analysis:\nâ€¢ Total Elapsed Time: {elapsed_time:.1f} seconds\nâ€¢ CPU Consumption: {cpu_time:.1f} seconds\nâ€¢ Execution Count: {executions:,} operations\nâ€¢ Per-Operation Time: {elapsed_per_exec:.3f}s average\n\nâš ï¸ Risk Factors:\nâ€¢ Background maintenance affecting production workload\nâ€¢ Resource contention during backup window\nâ€¢ Potential impact on concurrent user transactions\nâ€¢ Extended maintenance window duration\n\nðŸŽ¯ Priority: Schedule optimization and resource management required",
                    
                    "risk_level": "HIGH",
                    
                    "exact_commands": f"â° RMAN/Maintenance Operation Management\n\nðŸŽ¯ Immediate Actions for SQL_ID {sql_id}:\n   â€¢ REVIEW backup scheduling to minimize production impact\n   â€¢ EXEC RMAN: CONFIGURE DEVICE TYPE DISK PARALLELISM 2\n   â€¢ SCHEDULE maintenance during low-activity windows (2-6 AM)\n   â€¢ MONITOR concurrent session impact during backup operations\n\nðŸ“Š Performance Optimization:\n   â€¢ SET backup compression: CONFIGURE COMPRESSION ALGORITHM 'MEDIUM'\n   â€¢ LIMIT backup I/O impact: CONFIGURE BACKUP OPTIMIZATION ON\n   â€¢ REVIEW backup retention policy to reduce backup duration\n\nðŸ” Monitoring Commands:\n   â€¢ SELECT * FROM V$BACKUP_ASYNC_IO WHERE STATUS='RUNNING'\n   â€¢ SELECT * FROM V$SESSION_LONGOPS WHERE OPNAME LIKE '%RMAN%'\n   â€¢ MONITOR backup completion: SELECT * FROM V$RMAN_STATUS\n\nâš¡ Resource Management:\n   â€¢ Consider dedicated backup server if feasible\n   â€¢ IMPLEMENT Resource Manager to limit backup CPU usage\n   â€¢ SCHEDULE incremental backups during peak hours instead",
                    
                    "intelligence_condition": "CONFIRMED_MAINTENANCE"
                }
            
            # SCENARIO 2: Confirmed CPU bottleneck  
            elif cpu_time > 50 and elapsed_time > 80 and cpu_time / elapsed_time > 0.8:
                
                return {
                    "recommended_indexes": f"ðŸ”§ CPU Bottleneck Index Analysis\nâ€¢ SQL_ID {sql_id}: CPU-intensive query detected ({cpu_time:.1f}s CPU)\nâ€¢ ANALYZE execution plan: EXPLAIN PLAN FOR <query>\nâ€¢ CREATE selective indexes if full table scans identified\nâ€¢ REBUILD fragmented indexes: ALTER INDEX <index_name> REBUILD\nâ€¢ CONSIDER function-based indexes for WHERE clause functions\nâ€¢ MONITOR index usage: SELECT * FROM V$SQL_PLAN WHERE SQL_ID='{sql_id}'",
                    
                    "query_rewrite": f"ðŸ”§ CPU-Intensive Query Optimization\nâ€¢ SQL_ID {sql_id}: High computational overhead detected\nâ€¢ REVIEW query logic for CPU-intensive operations\nâ€¢ OPTIMIZE nested loops and computational functions\nâ€¢ CONSIDER breaking complex query into simpler operations\nâ€¢ REPLACE expensive functions with more efficient alternatives\nâ€¢ MINIMIZE data type conversions in WHERE clauses\nâ€¢ USE EXISTS instead of IN for better CPU efficiency",
                    
                    "risk_assessment": f"ðŸš¨ CRITICAL: SQL_ID {sql_id} - Confirmed CPU Bottleneck\n\nðŸ“Š CPU Performance Analysis:\nâ€¢ Total CPU Time: {cpu_time:.1f} seconds ({(cpu_time/elapsed_time*100):.1f}% of elapsed time)\nâ€¢ Total Elapsed Time: {elapsed_time:.1f} seconds\nâ€¢ Execution Count: {executions:,} executions\nâ€¢ CPU Per Execution: {(cpu_time/executions if executions > 0 else 0):.3f}s average\nâ€¢ Workload Impact: Consuming significant database CPU resources\n\nâš ï¸ Critical Risk Factors:\nâ€¢ CPU bottleneck affecting overall database performance\nâ€¢ Potential to cause CPU queue waits for other sessions\nâ€¢ Resource contention during peak hours\nâ€¢ Scalability concern with increased concurrent executions\n\nðŸŽ¯ Priority: Immediate SQL optimization and resource management required",
                    
                    "risk_level": "HIGH",
                    
                    "exact_commands": f"âš¡ CPU Bottleneck Resolution Strategy\n\nðŸŽ¯ Immediate Tuning Actions for SQL_ID {sql_id}:\n   â€¢ EXEC DBMS_SQLTUNE.CREATE_TUNING_TASK(sql_id=>'{sql_id}', task_name=>'TUNE_{sql_id}')\n   â€¢ EXEC DBMS_SQLTUNE.EXECUTE_TUNING_TASK('TUNE_{sql_id}')\n   â€¢ SELECT DBMS_SQLTUNE.REPORT_TUNING_TASK('TUNE_{sql_id}') FROM DUAL\n\nðŸ“Š Execution Plan Analysis:\n   â€¢ SELECT * FROM V$SQL_PLAN WHERE SQL_ID='{sql_id}' ORDER BY ID\n   â€¢ SELECT * FROM V$SQL_PLAN_STATISTICS WHERE SQL_ID='{sql_id}'\n   â€¢ ANALYZE table statistics: EXEC DBMS_STATS.GATHER_TABLE_STATS('<schema>','<table>')\n\nâš¡ Resource Management:\n   â€¢ CREATE Resource Manager consumer group for CPU-intensive queries\n   â€¢ EXEC DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP('HIGH_CPU_QUERIES')\n   â€¢ SET CPU limits: EXEC DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE\n\nðŸ” Performance Monitoring:\n   â€¢ MONITOR CPU queue waits: SELECT * FROM V$SYSTEM_EVENT WHERE EVENT='resmgr:cpu quantum'\n   â€¢ TRACK CPU usage: SELECT * FROM V$SYSMETRIC WHERE METRIC_NAME='CPU Usage Per Sec'\n   â€¢ SET alerts for CPU threshold breaches",
                    
                    "intelligence_condition": "CONFIRMED_CPU_BOTTLENECK"
                }
            
            # SCENARIO 3: Confirmed high-frequency issue
            elif executions > 1000 and elapsed_time > 10:
                
                return {
                    "recommended_indexes": f"ðŸ”§ High-Frequency Query Index Strategy\nâ€¢ SQL_ID {sql_id}: High execution frequency detected ({executions:,} executions)\nâ€¢ VERIFY index efficiency for repetitive operations\nâ€¢ MONITOR index usage: SELECT * FROM V$SQL_PLAN WHERE SQL_ID='{sql_id}'\nâ€¢ CONSIDER covering indexes to eliminate table access\nâ€¢ REVIEW index selectivity for optimal performance\nâ€¢ ENSURE proper index statistics are current",
                    
                    "query_rewrite": f"ðŸ”§ High-Frequency Query Optimization\nâ€¢ SQL_ID {sql_id}: Execution frequency optimization required\nâ€¢ IMPLEMENT result caching where business logic permits\nâ€¢ REVIEW application design for {executions:,} repeated calls\nâ€¢ CONSIDER batching multiple operations where possible\nâ€¢ ENSURE bind variables used consistently (no hard-coded literals)\nâ€¢ OPTIMIZE application connection pooling strategy\nâ€¢ EVALUATE stored procedure conversion for complex repeated logic",
                    
                    "risk_assessment": f"âš ï¸ MODERATE: SQL_ID {sql_id} - High Frequency Execution Pattern\n\nðŸ“Š Frequency Impact Analysis:\nâ€¢ Total Executions: {executions:,} operations\nâ€¢ Total Elapsed Time: {elapsed_time:.1f} seconds\nâ€¢ Average Per Execution: {elapsed_per_exec:.3f}s\nâ€¢ CPU Time: {cpu_time:.1f} seconds\nâ€¢ Frequency Impact: Moderate cumulative workload pressure\n\nðŸ“ˆ Performance Characteristics:\nâ€¢ Individual query performance: Acceptable ({elapsed_per_exec:.3f}s per execution)\nâ€¢ Cumulative impact: {elapsed_time:.1f}s total database time\nâ€¢ Execution pattern: High frequency may indicate application design issue\nâ€¢ Resource utilization: Monitor for session/connection overhead\n\nðŸŽ¯ Priority: Application optimization and caching strategy implementation",
                    
                    "risk_level": "MEDIUM",
                    
                    "exact_commands": f"ðŸ”„ High-Frequency Query Optimization Strategy\n\nðŸŽ¯ Application-Level Optimization for SQL_ID {sql_id}:\n   â€¢ REVIEW application code for {executions:,} repeated executions\n   â€¢ IMPLEMENT application-level result caching where appropriate\n   â€¢ CONSIDER database result cache: ALTER SYSTEM SET RESULT_CACHE_MODE=MANUAL\n   â€¢ OPTIMIZE connection pooling to reduce session overhead\n\nðŸ”§ Database-Level Optimization:\n   â€¢ MONITOR cursor sharing: SELECT * FROM V$SQL_SHARED_CURSOR WHERE SQL_ID='{sql_id}'\n   â€¢ VERIFY bind variable usage: SELECT * FROM V$SQL_BIND_CAPTURE WHERE SQL_ID='{sql_id}'\n   â€¢ CHECK session memory usage: SELECT * FROM V$SESSION WHERE SQL_ID='{sql_id}'\n\nðŸ“Š Performance Monitoring:\n   â€¢ SET up frequency alerts: Monitor executions per hour\n   â€¢ TRACK cursor cache efficiency: V$LIBRARYCACHE statistics\n   â€¢ MONITOR session counts and connection pooling effectiveness\n\nðŸ—ï¸ Architecture Review:\n   â€¢ EVALUATE micro-services design patterns\n   â€¢ CONSIDER read replicas for query distribution\n   â€¢ IMPLEMENT smart caching layers (Redis/Memcached)",
                    
                    "intelligence_condition": "CONFIRMED_HIGH_FREQUENCY"
                }
            
            else:
                # Should not reach here, but fallback just in case
                return self._generate_legacy_recommendations(sql_id, query_data, csv_data)
                
        except Exception as e:
            # Always fallback on error
            return self._generate_legacy_recommendations(sql_id, query_data, csv_data)

    def _load_csv_data(self, username="Siddhesh"):
        # type: () -> Dict[str, Any]
        """Load and prepare AWR CSV data for analysis"""
        import os
        import pandas as pd
        import glob
        
        try:
            # Use user-specific CSV directory
            csv_dir = "data/users/{}/parsed_csv".format(username)  # type: str
            
            # Load SQL stats
            sql_stats_file = self._find_csv_file(csv_dir, "awr_sql_stats")
            sql_stats = pd.read_csv(sql_stats_file) if sql_stats_file else None
            
            # Load wait events  
            wait_events_file = self._find_csv_file(csv_dir, "awr_wait_events")
            wait_events = pd.read_csv(wait_events_file) if wait_events_file else None
            
            # Load instance stats
            instance_stats_file = self._find_csv_file(csv_dir, "awr_instance_stats") 
            instance_stats = pd.read_csv(instance_stats_file) if instance_stats_file else None
            return {
                "sql_stats": sql_stats,
                "wait_events": wait_events,
                "instance_stats": instance_stats
            }
        except Exception as e:
            print(f"Error loading CSV data: {e}")
            return {"sql_stats": None, "wait_events": None, "instance_stats": None}
    
    def _find_csv_file(self, csv_dir, pattern):
        # type: (str, str) -> Optional[str]
        """Find CSV file matching pattern"""
        import os
        import glob
        
        search_pattern = os.path.join(csv_dir, "*{}*.csv".format(pattern))  # type: str
        matches = glob.glob(search_pattern)  # type: List[str]
        return matches[0] if matches else None
    
    def _build_query_context(self, sql_id: str, query_data: dict, csv_data: dict):
        """Build comprehensive context for analysis"""
        import pandas as pd
        import numpy as np
        
        # Extract basic metrics
        elapsed_time = float(query_data.get('elapsed', 0) or query_data.get('elapsed_time', 0))
        cpu_time = float(query_data.get('cpu', 0) or query_data.get('cpu_time', 0))  
        executions = int(query_data.get('executions', 0))
        elapsed_per_exec = float(query_data.get('elapsed_per_exec', 0))
        
        # Analyze SQL stats data for patterns
        sql_stats = csv_data.get("sql_stats")
        total_elapsed = 0
        total_executions = 0
        max_cpu_pct = 0
        high_elapsed_queries = 0
        
        if sql_stats is not None:
            sql_stats.columns = sql_stats.columns.str.strip().str.lower()
            
            # Clean and aggregate data
            elapsed_col = self._find_column(sql_stats, ["elapsed__time_s", "elapsed_time_s"])
            exec_col = self._find_column(sql_stats, ["executions"])
            cpu_col = self._find_column(sql_stats, ["pctcpu"])
            
            if elapsed_col:
                sql_stats[elapsed_col] = pd.to_numeric(sql_stats[elapsed_col], errors="coerce").fillna(0)
                total_elapsed = sql_stats[elapsed_col].sum()
                high_elapsed_queries: int = len(sql_stats[sql_stats[elapsed_col] > 10])
                
            if exec_col:
                sql_stats[exec_col] = pd.to_numeric(sql_stats[exec_col], errors="coerce").fillna(0) 
                total_executions = sql_stats[exec_col].sum()
                
            if cpu_col:
                sql_stats[cpu_col] = pd.to_numeric(sql_stats[cpu_col], errors="coerce").fillna(0)
                max_cpu_pct = sql_stats[cpu_col].max()
        
        # Analyze wait events for bottlenecks
        wait_events = csv_data.get("wait_events")
        db_cpu_time = 0
        major_wait_events = []
        
        if wait_events is not None:
            wait_events.columns = wait_events.columns.str.strip().str.lower()
            
            # Find DB CPU time
            db_cpu_row = wait_events[wait_events['statistic_name'].str.contains('DB CPU', na=False)]
            if not db_cpu_row.empty:
                time_col = self._find_column(wait_events, ["time_s"])
                if time_col:
                    db_cpu_time = pd.to_numeric(db_cpu_row[time_col].iloc[0], errors="coerce") or 0
            
            # Find major wait events (>10% DB time)
            pct_db_col = self._find_column(wait_events, ["pct_of__db_time", "pct_of_db_time"])
            if pct_db_col:
                wait_events[pct_db_col] = pd.to_numeric(wait_events[pct_db_col], errors="coerce").fillna(0)
                major_waits = wait_events[wait_events[pct_db_col] > 10]
                major_wait_events = major_waits['statistic_name'].tolist()
        
        return {
            # Query specific
            "sql_id": sql_id,
            "elapsed_time": elapsed_time,
            "cpu_time": cpu_time,
            "executions": executions,
            "elapsed_per_exec": elapsed_per_exec,
            
            # Workload context 
            "total_elapsed": total_elapsed,
            "total_executions": total_executions,
            "max_cpu_pct": max_cpu_pct,
            "high_elapsed_queries": high_elapsed_queries,
            "db_cpu_time": db_cpu_time,
            "major_wait_events": major_wait_events,
            
            # Raw data
            "csv_data": csv_data
        }
    
    def _find_column(self, df, candidates):
        """Find column by trying multiple name candidates"""
        for col in candidates:
            if col in df.columns:
                return col
        return None
    def _generate_intelligent_index_recommendations(self, context: dict) -> str:
        """ðŸŸ¢ 1ï¸âƒ£ Recommended Index - Only show when truly needed"""
        
        elapsed_time = context["elapsed_time"]
        executions = context["executions"] 
        elapsed_per_exec = context["elapsed_per_exec"]
        cpu_time = context["cpu_time"]
        sql_id = context["sql_id"]
        
        # Only show index recommendations when data suggests index issues
        
        # HIGH FREQUENCY + LOW IMPACT = Focus on efficiency not indexes
        if (executions > 800 and elapsed_time < 15 and cpu_time < 10 and 
            elapsed_per_exec < 0.2):
            return f"â€¢ MONITOR index usage for SQL_ID {sql_id} ({executions} executions)\nâ€¢ Current indexes appear adequate for individual query performance\nâ€¢ Focus on reducing execution frequency rather than index changes"
        
        # HIGH ELAPSED + LOW EXECUTIONS = Missing index scenario
        elif elapsed_time > 30 and executions < 200:
            return f"â€¢ CREATE missing indexes for SQL_ID {sql_id} ({elapsed_time:.1f}s elapsed)\nâ€¢ ANALYZE execution plan for full table scans\nâ€¢ CONSIDER composite indexes for multi-column WHERE conditions"
        
        # HIGH EXECUTIONS + SLOW PER-EXECUTION = Index optimization needed
        elif executions > 1000 and elapsed_per_exec > 0.05:
            return f"â€¢ OPTIMIZE indexes for high-volume query (SQL_ID: {sql_id}, {executions} executions)\nâ€¢ CREATE covering indexes to eliminate table lookups\nâ€¢ REVIEW existing index selectivity"
        
        # CPU-bound with many executions = Index efficiency issue
        elif cpu_time > 20 and executions > 500:
            return f"â€¢ ADD selective indexes to reduce CPU consumption\nâ€¢ REBUILD fragmented indexes for SQL_ID {sql_id}\nâ€¢ CONSIDER function-based indexes if using functions in WHERE"
        
        # Normal performance - minimal guidance
        else:
            return "Current index structure appears adequate for this query"

    def _generate_intelligent_query_rewrite(self, context: dict) -> str:
        """ðŸŸ  2ï¸âƒ£ Query Rewrite - Only when query itself is the problem"""
        
        elapsed_time = context["elapsed_time"] 
        cpu_time = context["cpu_time"]
        executions = context["executions"]
        elapsed_per_exec = context["elapsed_per_exec"]
        sql_id = context["sql_id"]
        
        # HIGH FREQUENCY + LOW IMPACT = Focus on caching and batching
        if (executions > 800 and elapsed_time < 15 and cpu_time < 10 and 
            elapsed_per_exec < 0.2):
            return f"â€¢ IMPLEMENT result caching for SQL_ID {sql_id} ({executions} executions)\nâ€¢ BATCH multiple calls if possible\nâ€¢ REVIEW application logic for excessive query triggering\nâ€¢ CONSIDER stored procedure for repeated operations"
        
        # HIGH ELAPSED + LOW EXECUTIONS = Query structure problem
        elif elapsed_time > 50 and executions < 100:
            return f"â€¢ REWRITE SQL_ID {sql_id} - query taking {elapsed_time:.1f}s per run\nâ€¢ OPTIMIZE JOIN order and conditions\nâ€¢ BREAK DOWN complex query into smaller operations\nâ€¢ ELIMINATE unnecessary subqueries and sorting"
        
        # VERY SLOW PER EXECUTION = Query logic inefficient  
        elif elapsed_per_exec > 0.5:
            return f"â€¢ IMPROVE query efficiency - {elapsed_per_exec:.2f}s per execution is high\nâ€¢ REVIEW WHERE clause selectivity\nâ€¢ REPLACE correlated subqueries with JOINs\nâ€¢ USE EXISTS instead of IN for better performance"
        
        # CPU-intensive query needing optimization
        elif cpu_time > elapsed_time * 0.8 and elapsed_time > 10:
            return f"â€¢ OPTIMIZE CPU-intensive operations in SQL_ID {sql_id}\nâ€¢ REDUCE computational complexity in SELECT clauses\nâ€¢ SIMPLIFY CASE statements and functions"
        
        # Normal performance - no rewrite needed
        else:
            return "Query structure appears optimized for current workload"

    def _calculate_intelligent_risk_score(self, context: dict) -> dict:
        """ðŸ”´ 3ï¸âƒ£ Risk Score - Real calculation with specific explanation"""
        
        elapsed_time = context["elapsed_time"]
        cpu_time = context["cpu_time"] 
        executions = context["executions"]
        elapsed_per_exec = context["elapsed_per_exec"]
        sql_id = context["sql_id"]
        
        risk_factors = []
        risk_points = 0
        
        # Specific risk calculation for THIS query
        
        # 1. Query impact severity
        if elapsed_time > 100:
            risk_points += 40
            risk_factors.append(f"SQL_ID {sql_id}: Critical elapsed time {elapsed_time:.1f}s")
        elif elapsed_time > 30:
            risk_points += 20
            risk_factors.append(f"SQL_ID {sql_id}: High elapsed time {elapsed_time:.1f}s")
        
        # 2. Per-execution efficiency
        if elapsed_per_exec > 1.0:
            risk_points += 30
            risk_factors.append(f"Very slow per execution: {elapsed_per_exec:.2f}s per run")
        elif elapsed_per_exec > 0.2:
            risk_points += 15
            risk_factors.append(f"Slow per execution: {elapsed_per_exec:.2f}s per run")
            
        # 3. CPU consumption analysis
        if cpu_time > 50:
            risk_points += 25
            risk_factors.append(f"Very high CPU usage: {cpu_time:.1f}s")
        elif cpu_time > 15:
            risk_points += 10
            risk_factors.append(f"High CPU usage: {cpu_time:.1f}s")
            
        # 4. Frequency impact
        if executions > 1000 and elapsed_time > 10:
            risk_points += 20
            risk_factors.append(f"High-volume problematic query: {executions} executions")
        
        # 5. Query efficiency ratio
        if cpu_time > 0 and elapsed_time > cpu_time * 3:
            risk_points += 15
            risk_factors.append("Significant I/O wait indicating missing indexes")
        
        # 6. NEW RULE: High Frequency + Low Individual Impact = MEDIUM Risk
        # This catches queries that are individually harmless but create workload pressure
        if (executions > 800 and elapsed_time < 15 and cpu_time < 10 and 
            elapsed_per_exec < 0.2):
            risk_points += 30  # Force MEDIUM risk
            risk_factors.append(f"High execution frequency detected: {executions} runs")
            risk_factors.append("Individual query performance acceptable but frequency creates workload pressure")
            
        # Determine risk level based on actual impact
        if risk_points >= 60:
            risk_level = "HIGH"
            explanation: str = f"CRITICAL: SQL_ID {sql_id} requires immediate attention"
        elif risk_points >= 25:
            risk_level = "MEDIUM" 
            explanation: str = f"MODERATE: SQL_ID {sql_id} needs optimization soon"
        else:
            risk_level = "LOW"
            explanation: str = f"ACCEPTABLE: SQL_ID {sql_id} performance within range"
        
        # Special handling for high frequency + low impact queries
        if (executions > 800 and elapsed_time < 15 and cpu_time < 10 and 
            elapsed_per_exec < 0.2):
            risk_level = "MEDIUM"
            explanation: str = f"MEDIUM IMPACT: SQL_ID {sql_id} - High execution frequency detected"
            assessment: str = f"Query does not consume high CPU or elapsed individually, but repeated execution may create workload pressure.\n\nFrequency Analysis for {sql_id}:\nâ€¢ {executions} executions detected\nâ€¢ Individual performance acceptable ({elapsed_time:.1f}s elapsed, {cpu_time:.1f}s CPU)\nâ€¢ Workload impact from high frequency execution pattern"
        else:
            # Build specific assessment for other scenarios
            if risk_factors:
                assessment: str = f"{explanation}\n\nSpecific Issues for {sql_id}:\n" + "\n".join([f"â€¢ {factor}" for factor in risk_factors])
            else:
                assessment: str = f"SQL_ID {sql_id}: No critical performance issues detected"
            
        return {
            "level": risk_level,
            "assessment": assessment,
            "risk_points": risk_points
        }

    def _generate_exact_dba_recommendations(self, context: dict) -> str:
        """ðŸ”µ 4ï¸âƒ£ Exact DBA / System Recommendations - Only relevant sections"""
        
        elapsed_time = context["elapsed_time"]
        cpu_time = context["cpu_time"]
        executions = context["executions"] 
        elapsed_per_exec = context["elapsed_per_exec"]
        sql_id = context["sql_id"]
        
        recommendations = []
        
        # Only show relevant recommendations based on actual query characteristics
        
        # NEW SCENARIO: High Frequency + Low Impact (Workload pressure from frequency)
        if (executions > 800 and elapsed_time < 15 and cpu_time < 10 and 
            elapsed_per_exec < 0.2):
            recommendations.append(f"âš¡ Execution Burst / Frequency Optimization\n   â€¢ SQL_ID {sql_id}: Check if triggered repeatedly in application loops\n   â€¢ Reduce execution frequency through business logic optimization\n   â€¢ Implement result caching for repeated value queries\n   â€¢ Review application design for excessive micro-calls")
            
            recommendations.append("ðŸ”§ Bind Variables & Cursor Sharing\n   â€¢ Ensure bind variables are used instead of literals\n   â€¢ Monitor cursor sharing efficiency: V$SQL_SHARED_CURSOR\n   â€¢ Check for cursor cache misses\n   â€¢ Optimize SQL structure for better reuse")
            
            recommendations.append("ðŸ—ï¸ Application / Architecture Optimization\n   â€¢ Avoid too many micro SQL calls from application\n   â€¢ Implement batching strategy for bulk operations\n   â€¢ Reduce background polling frequency if applicable\n   â€¢ Consider connection pooling optimization")
            
        # SCENARIO 1: Query-heavy problem (High elapsed + Low executions)
        elif elapsed_time > 30 and executions < 200:
            recommendations.append(f"ðŸŽ¯ Tune This Specific SQL\n   â€¢ SQL_ID {sql_id}: Focus on query rewrite and execution plan\n   â€¢ Use SQL Tuning Advisor: EXEC DBMS_SQLTUNE.CREATE_TUNING_TASK(sql_id=>'{sql_id}')\n   â€¢ Analyze execution plan: SELECT * FROM V$SQL_PLAN WHERE SQL_ID = '{sql_id}'")
            
            recommendations.append("ðŸ“Š Add Missing Indexes\n   â€¢ Run SQL Access Advisor for this specific query\n   â€¢ Check for full table scans in execution plan\n   â€¢ CREATE indexes for WHERE clause columns")
        
        # SCENARIO 2: Workload problem (High CPU + High executions)  
        elif cpu_time > 20 and executions > 500:
            recommendations.append("âš¡ Control CPU and Workload\n   â€¢ Implement Resource Manager to limit CPU for this query type\n   â€¢ EXEC DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP\n   â€¢ Consider connection pooling optimization")
            
            recommendations.append("ðŸ”§ Optimize Statistics\n   â€¢ Update table statistics: EXEC DBMS_STATS.GATHER_TABLE_STATS\n   â€¢ Check for stale statistics affecting optimizer\n   â€¢ Set proper histogram collection")
            
            recommendations.append("ðŸ”„ Session Optimization\n   â€¢ Monitor cursor sharing for this high-volume query\n   â€¢ Convert literals to bind variables\n   â€¢ Optimize connection management")
        
        # SCENARIO 3: I/O bound query (High elapsed, lower CPU)
        elif elapsed_time > 20 and cpu_time < elapsed_time * 0.5:
            recommendations.append(f"ðŸ’¾ Index and I/O Optimization\n   â€¢ SQL_ID {sql_id}: Focus on reducing I/O waits\n   â€¢ Add indexes for better data access\n   â€¢ Consider table partitioning if large table access")
            
            recommendations.append("ðŸ“ˆ Update Statistics\n   â€¢ Refresh table statistics for better execution plans\n   â€¢ EXEC DBMS_STATS.GATHER_SCHEMA_STATS(estimate_percent=>10)")
        
        # SCENARIO 4: RMAN/Maintenance query detected
        elif "rman" in sql_id.lower() or elapsed_time > 60:
            recommendations.append("â° Background Job Management\n   â€¢ Schedule maintenance tasks during low-activity periods\n   â€¢ Adjust RMAN backup parallelism\n   â€¢ Monitor backup impact on production workload")
        
        # SCENARIO 5: Low risk - minimal guidance
        elif elapsed_time < 10 and cpu_time < 5 and executions < 500:
            recommendations.append("âœ… Continue Monitoring\n   â€¢ Current performance acceptable\n   â€¢ Set up alerts if performance degrades\n   â€¢ Monitor execution frequency changes")
        
        # Default for edge cases
        else:
            recommendations.append(f"ðŸ” Analyze SQL_ID {sql_id}\n   â€¢ Review execution plan for optimization opportunities\n   â€¢ Monitor performance trends\n   â€¢ Consider query rewrite if performance degrades")
        
        # Always add monitoring for any problematic query
        if elapsed_time > 10 or executions > 1000:
            recommendations.append("ðŸ“Š Set Up Monitoring\n   â€¢ Track this query's performance over time\n   â€¢ Set alerts: V$SYSMETRIC, V$SESSION_LONGOPS\n   â€¢ Include in regular AWR analysis")
        
        return "\n\n".join(recommendations)

    def _generate_intelligent_recommendations(self, sql_id: str, query_data: dict, csv_data: dict) -> dict:
        """Generate recommendations using the new SQL Intelligence Engine"""
        
        try:
            # Initialize the intelligence engine
            intelligence_engine = SQLIntelligenceEngine(csv_data)
            
            # Get SQL text from the CSV data
            sql_text: str = self._get_sql_text_for_query(sql_id, csv_data)
            
            # Prepare metrics for analysis
            metrics = {
                'elapsed_time': float(query_data.get('elapsed', 0) or query_data.get('elapsed_time', 0)),
                'cpu_time': float(query_data.get('cpu', 0) or query_data.get('cpu_time', 0)),
                'executions': int(query_data.get('executions', 0)),
                'elapsed_per_exec': float(query_data.get('elapsed_per_exec', 0)),
                'pcttotal': float(query_data.get('pcttotal', 0)),
                'pctcpu': float(query_data.get('pctcpu', 0)),
                'pctio': float(query_data.get('pctio', 0))
            }
            
            # Run intelligent analysis
            analysis_result: Dict[str, Any] = intelligence_engine.analyze_sql_patterns(sql_id, sql_text, metrics)
            
            # Format results for the UI (maintaining exact same structure)
            return {
                "recommended_indexes": analysis_result.get('index_recommendations', 'No index recommendations available'),
                "query_rewrite": analysis_result.get('query_rewrite', 'No query rewrite suggestions available'),
                "risk_assessment": analysis_result.get('risk_assessment', 'Risk assessment not available'),
                "risk_level": analysis_result.get('risk_level', 'UNKNOWN'),
                "exact_commands": analysis_result.get('dba_recommendations', 'No specific commands available'),
                # Additional metadata for debugging (not shown in UI)
                "intelligence_condition": analysis_result.get('condition'),
                "patterns_detected": analysis_result.get('patterns_detected', [])
            }
            
        except Exception as e:
            # Fall back to legacy recommendations if intelligence engine fails
            print(f"Intelligence engine failed for SQL_ID {sql_id}: {e}")
            return self._generate_legacy_recommendations(sql_id, query_data, csv_data)
    
    def _get_sql_text_for_query(self, sql_id: str, csv_data: dict) -> str:
        """Extract SQL text for the specific SQL_ID from CSV data"""
        try:
            sql_stats = csv_data.get('sql_stats')
            if sql_stats is None:
                return ""
            
            # Find the row with matching SQL_ID
            sql_id_col = self._find_column(sql_stats, ['sql_id'])
            sql_text_col = self._find_column(sql_stats, ['sql_text'])
            
            if sql_id_col and sql_text_col:
                matching_row = sql_stats[sql_stats[sql_id_col] == sql_id]
                if not matching_row.empty:
                    return str(matching_row[sql_text_col].iloc[0])
            
            return ""
        except Exception as e:
            print(f"Error extracting SQL text for {sql_id}: {e}")
            return ""
    
    def _generate_minimal_fallback(self, sql_id: str, query_data: dict, error_msg: str) -> dict:
        """
        Minimal fallback when decision engine fails.
        
        CRITICAL: This does NOT contain templated SQL.
        It returns a minimal response asking for manual review.
        """
        elapsed_time = float(query_data.get('elapsed', 0) or 0)
        cpu_time = float(query_data.get('cpu', 0) or 0)
        executions = int(query_data.get('executions', 0) or 0)
        
        return {
            "recommended_indexes": f"Manual analysis required for SQL_ID {sql_id}",
            "query_rewrite": "Automated analysis failed - manual review recommended",
            "risk_assessment": (
                f"âš ï¸ ANALYSIS ERROR for SQL_ID {sql_id}\n\n"
                f"Error: {error_msg}\n\n"
                f"Available Metrics:\n"
                f"â€¢ Elapsed: {elapsed_time:.1f}s\n"
                f"â€¢ CPU: {cpu_time:.1f}s\n"
                f"â€¢ Executions: {executions:,}\n\n"
                "Manual DBA review required."
            ),
            "risk_level": "UNKNOWN",
            "exact_commands": (
                f"-- Manual Analysis Required for SQL_ID: {sql_id}\n"
                f"-- Reason: Automated decision engine encountered an error\n"
                f"-- Action: DBA should manually analyze this SQL"
            ),
            "dba_action_plan": "",
            "action_plan_data": {},
            "why_shown": ["Fallback mode - automated analysis failed"],
            "why_hidden": ["All automated recommendations suppressed due to error"],
            "decision_category": "ERROR_FALLBACK",
            "allowed_actions": [],
            "blocked_actions": [],
            "reasoning": [f"Error: {error_msg}"],
            "signal_fingerprint": f"error|{sql_id}"
        }
    
    def _generate_legacy_recommendations(self, sql_id: str, query_data: dict, csv_data: dict) -> dict:
        """Generate evidence-based recommendations using proven original logic"""
        
        try:
            # Extract metrics
            elapsed_time = float(query_data.get('elapsed', 0) or query_data.get('elapsed_time', 0))
            cpu_time = float(query_data.get('cpu', 0) or query_data.get('cpu_time', 0))
            executions = int(query_data.get('executions', 0))
            elapsed_per_exec = float(query_data.get('elapsed_per_exec', 0))
            
            # RMAN/MAINTENANCE DETECTION
            sql_module: str = str(query_data.get('sql_module', '')).lower()
            is_rman: bool = ('rman' in sql_module or 'backup' in sql_module or 
                      'maintenance' in sql_module or 'dbms_backup' in sql_module)
            
            # EVIDENCE-BASED INDEX RECOMMENDATIONS
            index_recommendations = []
            
            # Special handling for RMAN/maintenance operations
            if is_rman:
                index_recommendations.append(f"â€¢ RMAN/Maintenance Operation Detected for SQL_ID {sql_id}")
                index_recommendations.append("â€¢ Index optimization not applicable for backup operations")
                index_recommendations.append("â€¢ Focus on backup scheduling and parallelism instead")
            # Scenario 1: High elapsed per execution suggests missing indexes
            elif elapsed_per_exec > 0.5:
                index_recommendations.append(f"â€¢ ANALYZE execution plan for SQL_ID {sql_id} (slow per execution: {elapsed_per_exec:.2f}s)")
                index_recommendations.append("â€¢ CREATE selective indexes for WHERE clause conditions if full table scans detected")
            
            # Scenario 2: High total elapsed with moderate executions
            elif elapsed_time > 30 and executions < 500:
                index_recommendations.append(f"â€¢ REVIEW execution plan for SQL_ID {sql_id}")
                index_recommendations.append("â€¢ ADD indexes for frequent filter conditions")
            
            # Scenario 3: High frequency but reasonable performance
            elif executions > 500 and elapsed_per_exec < 0.1:
                index_recommendations.append(f"â€¢ MONITOR existing index usage for SQL_ID {sql_id}")
                index_recommendations.append("â€¢ Current index structure appears adequate")
            
            # Scenario 4: Normal performance
            else:
                index_recommendations.append("â€¢ No immediate index changes required")
                index_recommendations.append("â€¢ Continue monitoring access patterns")
            
            # EVIDENCE-BASED QUERY REWRITE RECOMMENDATIONS  
            rewrite_recommendations = []
            
            # Special handling for RMAN/maintenance operations  
            if is_rman:
                rewrite_recommendations.append(f"â€¢ RMAN/Maintenance Operation for SQL_ID {sql_id}")
                rewrite_recommendations.append("â€¢ Query rewrite not applicable for system-generated backup SQL")
                rewrite_recommendations.append("â€¢ Consider RMAN parallelism and scheduling optimization instead")
            # High elapsed per execution
            elif elapsed_per_exec > 1.0:
                rewrite_recommendations.append(f"â€¢ OPTIMIZE query structure for SQL_ID {sql_id}")
                rewrite_recommendations.append("â€¢ Review WHERE clause selectivity")
                rewrite_recommendations.append("â€¢ Consider breaking down complex operations")
            
            # High frequency pattern
            elif executions > 800:
                rewrite_recommendations.append(f"â€¢ REVIEW application logic calling SQL_ID {sql_id}")
                rewrite_recommendations.append("â€¢ Consider result caching where appropriate")
                rewrite_recommendations.append("â€¢ Ensure bind variables are used consistently")
                
            # CPU intensive
            elif cpu_time > 15:
                rewrite_recommendations.append(f"â€¢ REVIEW computational complexity in SQL_ID {sql_id}")
                rewrite_recommendations.append("â€¢ Optimize functions and calculations")
                
            # Normal performance  
            else:
                rewrite_recommendations.append("â€¢ Query performance acceptable")
                rewrite_recommendations.append("â€¢ No immediate rewrite required")
            
            # EVIDENCE-BASED RISK ASSESSMENT with FIXED LOGIC
            risk_factors = []
            risk_details = []
            
            # FIX 2ï¸âƒ£: Use STRICT Risk Classification Logic  
            elapsed_time = float(query_data.get('elapsed', 0))
            cpu_time = float(query_data.get('cpu', 0)) 
            executions = int(query_data.get('executions', 0))
            pctcpu = float(query_data.get('pctcpu', 0))
            pcttotal = float(query_data.get('pcttotal', 0))
            
            # STRICT RISK CATEGORY RULES
            if elapsed_time >= 60 or cpu_time >= 60:
                risk_level = "HIGH"
                risk_category = "CRITICAL"
            elif executions >= 700:
                risk_level = "MEDIUM"
                risk_category = "MODERATE"
            else:
                risk_level = "LOW"
                risk_category = "ACCEPTABLE"
            
            # FORMAT EXACTLY AS SPECIFIED
            risk_assessment: str = f"{risk_category}: SQL_ID {sql_id} requires immediate attention\nSpecific Issues for {sql_id}:"
            
            # Add specific issues based on actual data
            if elapsed_time >= 60:
                risk_assessment += f"\nâ€¢ Critical elapsed time: {elapsed_time:.1f}s"
            elif elapsed_time > 30:
                risk_assessment += f"\nâ€¢ High elapsed time: {elapsed_time:.1f}s"
            elif elapsed_time > 10:
                risk_assessment += f"\nâ€¢ Moderate elapsed time: {elapsed_time:.1f}s"
                
            if cpu_time >= 60:
                risk_assessment += f"\nâ€¢ Very high CPU usage: {cpu_time:.1f}s"
            elif cpu_time > 20:
                risk_assessment += f"\nâ€¢ High CPU usage: {cpu_time:.1f}s"
            elif cpu_time > 5:
                risk_assessment += f"\nâ€¢ Moderate CPU usage: {cpu_time:.1f}s"
                
            if executions >= 700:
                risk_assessment += f"\nâ€¢ High frequency executions: {executions:,}"
            elif executions > 100:
                risk_assessment += f"\nâ€¢ Executions: {executions:,}"
            
            # EVIDENCE-BASED DBA COMMANDS
            dba_commands = []
            
            # High elapsed time scenarios
            if elapsed_time > 50:
                dba_commands.append(f"ðŸŽ¯ High-Impact SQL Optimization\n\nðŸ“Š Performance Analysis for SQL_ID {sql_id}:\n   â€¢ Total Database Time: {elapsed_time:.1f} seconds\n   â€¢ Optimization Priority: HIGH\n\nðŸ”§ Immediate Actions:\n   â€¢ EXEC DBMS_SQLTUNE.CREATE_TUNING_TASK(sql_id=>'{sql_id}', task_name=>'TUNE_{sql_id[:8]}')\n   â€¢ EXEC DBMS_SQLTUNE.EXECUTE_TUNING_TASK('TUNE_{sql_id[:8]}')\n   â€¢ SELECT DBMS_SQLTUNE.REPORT_TUNING_TASK('TUNE_{sql_id[:8]}') FROM DUAL")
            
            # High frequency scenarios
            if executions > 500:
                per_exec_text: str = ""
                if elapsed_per_exec > 0.001:  # Only show if meaningful value
                    per_exec_text: str = f"\n   â€¢ Per-Execution Time: {elapsed_per_exec:.3f}s average"
                
                dba_commands.append(f"ðŸ”„ High-Frequency Query Management\n\nðŸ“Š Frequency Analysis for SQL_ID {sql_id}:\n   â€¢ Execution Count: {executions:,} operations{per_exec_text}\n\nðŸ”§ Frequency Optimization:\n   â€¢ MONITOR cursor sharing: SELECT * FROM V$SQL_SHARED_CURSOR WHERE SQL_ID='{sql_id}'\n   â€¢ VERIFY bind variables: SELECT * FROM V$SQL_BIND_CAPTURE WHERE SQL_ID='{sql_id}'")
            
            # CPU scenarios - REMOVE (0.0% of DB CPU) text
            if cpu_time > 15 or pctcpu > 50:
                dba_commands.append(f"âš¡ CPU Optimization Strategy\n\nðŸ“Š CPU Analysis for SQL_ID {sql_id}:\n   â€¢ CPU Usage: {cpu_time:.1f}s\n\nðŸ”§ CPU Optimization Actions:\n   â€¢ UPDATE table statistics: EXEC DBMS_STATS.GATHER_TABLE_STATS('<schema>','<table>')")
            
            # Standard monitoring for stable queries
            if risk_level == "LOW":
                dba_commands.append(f"ðŸ“ˆ Performance Monitoring for SQL_ID {sql_id}:\n   â€¢ Performance: Within acceptable parameters\n   â€¢ Action Required: Routine monitoring\n   â€¢ SELECT * FROM DBA_HIST_SQLSTAT WHERE SQL_ID='{sql_id}'")
            
            return {
                "recommended_indexes": "\n".join(index_recommendations) if index_recommendations else "No index changes recommended",
                "query_rewrite": "\n".join(rewrite_recommendations) if rewrite_recommendations else "No query rewrite needed",
                "risk_assessment": risk_assessment,
                "risk_level": risk_level,
                "exact_commands": "\n\n".join(dba_commands) if dba_commands else f"SQL_ID {sql_id}: Continue standard monitoring"
            }
            
        except Exception as e:
            # Ultimate fallback
            return {
                "recommended_indexes": "Analysis requires manual review",
                "query_rewrite": "Manual query analysis recommended", 
                "risk_assessment": f"SQL_ID {sql_id}: Unable to complete automated analysis",
                "risk_level": "UNKNOWN",
                "exact_commands": "Manual DBA analysis required"
            }

    def _generate_rman_maintenance_recommendations(self, sql_id: str, query_data: dict, risk_analysis: dict) -> dict:
        """STRICT RMAN/Maintenance recommendations - NO index tuning, realistic timing"""
        
        elapsed_time = risk_analysis['elapsed_time']
        executions = risk_analysis['executions']
        cpu_time = risk_analysis['cpu_time']
        
        recommendations = []
        
        # Backup/Maintenance specific guidance
        recommendations.append("ðŸ”§ **RMAN/MAINTENANCE OPTIMIZATION**")
        recommendations.append(f"   â€¢ Current Runtime: {elapsed_time:.1f}s total across {executions} executions")
        recommendations.append(f"   â€¢ Average per execution: {elapsed_time/max(executions,1):.2f}s")
        
        if elapsed_time > 120:
            recommendations.append("âš¡ **PARALLELISM TUNING**")
            recommendations.append("   â€¢ Review RMAN backup parallelism settings")
            recommendations.append("   â€¢ Consider adjusting RMAN CONFIGURE BACKUP parallelism")
            recommendations.append("   â€¢ Verify storage bandwidth capacity")
        
        if cpu_time > 30:
            recommendations.append("ðŸ“… **SCHEDULING OPTIMIZATION**")  
            recommendations.append("   â€¢ Schedule during low-activity periods")
            recommendations.append("   â€¢ Consider workload separation from production")
            recommendations.append("   â€¢ Monitor impact on concurrent user sessions")
        
        recommendations.append("ðŸ“Š **MONITORING FOCUS**")
        recommendations.append("   â€¢ Track backup completion times consistently") 
        recommendations.append("   â€¢ Monitor storage I/O during maintenance windows")
        recommendations.append("   â€¢ Verify backup integrity post-completion")
        
        return self._format_strict_response(sql_id, query_data, risk_analysis, recommendations, "RMAN/MAINTENANCE")

    def _generate_high_impact_recommendations(self, sql_id: str, query_data: dict, risk_analysis: dict, sql_text: str) -> dict:
        """HIGH IMPACT SQL recommendations with actual data"""
        
        elapsed_time = risk_analysis['elapsed_time']
        cpu_time = risk_analysis['cpu_time']
        pctcpu = risk_analysis['pctcpu']
        
        recommendations = []
        
        recommendations.append("ðŸš¨ **HIGH IMPACT SQL ANALYSIS**")
        recommendations.append(f"   â€¢ Total Elapsed: {elapsed_time:.1f}s")
        recommendations.append(f"   â€¢ CPU Time: {cpu_time:.1f}s ({pctcpu:.1f}% of total DB CPU)")
        
        if cpu_time > elapsed_time * 0.7:  # CPU dominant
            recommendations.append("ðŸ”¥ **CPU BOTTLENECK DETECTED**")
            recommendations.append("   â€¢ Run SQL Tuning Advisor immediately:")
            recommendations.append(f"   â€¢ EXEC DBMS_SQLTUNE.CREATE_TUNING_TASK(sql_id => '{sql_id}', task_name => 'TUNE_{sql_id}');")
            recommendations.append("   â€¢ Check for missing indexes or inefficient joins")
            recommendations.append("   â€¢ Review execution plan for full table scans")
        
        if elapsed_time > 60:
            recommendations.append("âš™ï¸ **PERFORMANCE TUNING REQUIRED**")
            recommendations.append("   â€¢ Generate detailed execution plan with timing:")
            recommendations.append(f"   â€¢ ALTER SESSION SET STATISTICS_LEVEL=ALL;")
            recommendations.append(f"   â€¢ Review actual vs estimated row counts")
            recommendations.append("   â€¢ Check for outdated table statistics")
        
        recommendations.append("ðŸŽ¯ **IMMEDIATE ACTIONS**")
        recommendations.append(f"   â€¢ SET AUTOTRACE ON EXPLAIN STAT for SQL_ID {sql_id}")
        recommendations.append("   â€¢ Capture 10053 trace for detailed optimizer decisions")
        recommendations.append("   â€¢ Review bind variable usage and cursor sharing")
        
        return self._format_strict_response(sql_id, query_data, risk_analysis, recommendations, "HIGH_IMPACT")

    def _generate_frequency_recommendations(self, sql_id: str, query_data: dict, risk_analysis: dict) -> dict:
        """High execution frequency recommendations - ALWAYS MEDIUM risk"""
        
        executions = risk_analysis['executions']
        elapsed_time = risk_analysis['elapsed_time']
        per_exec = elapsed_time / max(executions, 1)
        
        # STRICT RULE: Force MEDIUM risk for high frequency
        risk_analysis['level'] = 'MEDIUM'
        risk_analysis['factors'].append("High frequency execution pattern requires monitoring")
        
        recommendations = []
        
        recommendations.append("ðŸ“ˆ **HIGH FREQUENCY SQL PATTERN**")
        recommendations.append(f"   â€¢ Executions: {executions:,} times")
        recommendations.append(f"   â€¢ Per execution: {per_exec:.3f}s average")
        recommendations.append(f"   â€¢ Total impact: {elapsed_time:.1f}s cumulative")
        
        recommendations.append("ðŸ” **FREQUENCY ANALYSIS REQUIRED**")
        recommendations.append("   â€¢ Verify application logic for unnecessary repeated calls")
        recommendations.append("   â€¢ Check bind variable usage and cursor sharing")
        recommendations.append("   â€¢ Review connection pooling configuration")
        
        if per_exec < 0.1:
            recommendations.append("ðŸ’¾ **CONSIDER CACHING**")
            recommendations.append("   â€¢ Evaluate result set caching opportunities")
            recommendations.append("   â€¢ Review if data changes frequently enough to warrant constant querying")
            
        recommendations.append("ðŸ“Š **MONITORING SETUP**")
        recommendations.append("   â€¢ Set up alerts for execution count spikes")
        recommendations.append("   â€¢ Track cursor sharing efficiency")
        recommendations.append("   â€¢ Monitor session concurrency patterns")
        
        return self._format_strict_response(sql_id, query_data, risk_analysis, recommendations, "HIGH_FREQUENCY")

    def _generate_standard_recommendations(self, sql_id: str, query_data: dict, risk_analysis: dict) -> dict:
        """Standard SQL recommendations with actual data"""
        
        elapsed_time = risk_analysis['elapsed_time']
        cpu_time = risk_analysis['cpu_time']
        executions = risk_analysis['executions']
        
        recommendations = []
        
        recommendations.append("âœ… **STANDARD PERFORMANCE ANALYSIS**")
        recommendations.append(f"   â€¢ Current metrics: {elapsed_time:.1f}s elapsed, {executions:,} executions")
        recommendations.append(f"   â€¢ CPU usage: {cpu_time:.1f}s")
        
        if elapsed_time > 5:
            recommendations.append("ðŸ”§ **BASIC TUNING OPPORTUNITIES**")
            recommendations.append("   â€¢ Review execution plan for efficiency")
            recommendations.append("   â€¢ Verify table statistics are current")
            recommendations.append("   â€¢ Check for appropriate indexing")
        
        recommendations.append("ðŸ“‹ **ROUTINE MONITORING**")
        recommendations.append("   â€¢ Include in regular performance review")
        recommendations.append("   â€¢ Track trend over time")
        recommendations.append("   â€¢ No immediate action required")
        
        return self._format_strict_response(sql_id, query_data, risk_analysis, recommendations, "STANDARD")

    def _format_strict_response(self, sql_id: str, query_data: dict, risk_analysis: dict, recommendations: list, category: str) -> dict:
        """Format response with STRICT data presentation rules"""
        
        elapsed_time = risk_analysis['elapsed_time']
        cpu_time = risk_analysis['cpu_time'] 
        executions = risk_analysis['executions']
        per_exec = elapsed_time / max(executions, 1)
        
        # STRICT RISK PRESENTATION
        risk_section: str = f"""
**RISK ASSESSMENT: {risk_analysis['level']}**
- SQL_ID: {sql_id}
- Total Elapsed: {elapsed_time:.1f}s
- CPU Time: {cpu_time:.1f}s  
- Executions: {executions:,}
- Per Execution Average: {per_exec:.3f}s
- Risk Factors: {', '.join(risk_analysis['factors']) if risk_analysis['factors'] else 'Standard performance profile'}
"""
        
        # Combine all sections
        full_recommendations = [risk_section] + recommendations  # type: List[str]
        
        return {
            'sql_id': sql_id,
            'risk_level': risk_analysis['level'],
            'category': category,
            'recommended_indexes': f"Category: {category} - Index analysis based on actual performance data",
            'query_rewrite': f"Category: {category} - Query optimization recommendations", 
            'risk_assessment': '\n'.join(full_recommendations),
            'exact_commands': self._generate_dba_commands(sql_id, risk_analysis, category),
            'priority': 'High' if risk_analysis['level'] in ['HIGH', 'CRITICAL'] else 'Medium' if risk_analysis['level'] == 'MEDIUM' else 'Low',
            'estimated_improvement': self._estimate_improvement_realistic(risk_analysis)
        }

    def _generate_dba_commands(self, sql_id: str, risk_analysis: dict, category: str) -> str:
        """Generate practical DBA commands based on category and risk"""
        
        commands = []
        elapsed_time = risk_analysis['elapsed_time']
        cpu_time = risk_analysis['cpu_time']
        executions = risk_analysis['executions']
        
        if category == "RMAN/MAINTENANCE":
            commands.append(f"ðŸ”§ RMAN/Maintenance Commands for SQL_ID {sql_id}:")
            commands.append(f"   â€¢ CONFIGURE DEVICE TYPE DISK PARALLELISM 2;")
            commands.append(f"   â€¢ CONFIGURE BACKUP OPTIMIZATION ON;")
            commands.append(f"   â€¢ SHOW ALL; -- Review current RMAN configuration")
            
        elif category == "HIGH_IMPACT":
            commands.append(f"âš¡ High-Impact SQL Commands for SQL_ID {sql_id}:")
            commands.append(f"   â€¢ EXEC DBMS_SQLTUNE.CREATE_TUNING_TASK(sql_id=>'{sql_id}', task_name=>'TUNE_{sql_id[:8]}');")
            commands.append(f"   â€¢ SELECT * FROM V$SQL_PLAN WHERE SQL_ID='{sql_id}' ORDER BY ID;")
            commands.append(f"   â€¢ EXEC DBMS_STATS.GATHER_SCHEMA_STATS(ownname=>'<SCHEMA>', estimate_percent=>10);")
            
        elif category == "HIGH_FREQUENCY":
            commands.append(f"ðŸ”„ High-Frequency SQL Commands for SQL_ID {sql_id}:")
            commands.append(f"   â€¢ SELECT * FROM V$SQL_SHARED_CURSOR WHERE SQL_ID='{sql_id}';")
            commands.append(f"   â€¢ SELECT * FROM V$SQL_BIND_CAPTURE WHERE SQL_ID='{sql_id}';")
            commands.append(f"   â€¢ SELECT COUNT(*) FROM V$SESSION WHERE SQL_ID='{sql_id}';")
            
        else:
            commands.append(f"ðŸ“Š Standard Monitoring Commands for SQL_ID {sql_id}:")
            commands.append(f"   â€¢ SELECT * FROM V$SQL WHERE SQL_ID='{sql_id}';")
            commands.append(f"   â€¢ SELECT * FROM DBA_HIST_SQLSTAT WHERE SQL_ID='{sql_id}';")
        
        return '\n'.join(commands)

    def _estimate_improvement_realistic(self, risk_analysis: dict) -> str:
        """Realistic improvement estimates based on actual data"""
        
        if risk_analysis['level'] == 'CRITICAL':
            return "20-50% performance improvement possible with proper tuning"
        elif risk_analysis['level'] == 'HIGH':
            return "10-30% improvement expected with optimization"
        elif risk_analysis['level'] == 'MEDIUM':
            return "5-15% improvement possible, focus on frequency management"
        else:
            return "Minor optimization opportunities, maintain current monitoring"

    def _generate_error_fallback_recommendations(self, sql_id: str, query_data: dict, error_msg: str) -> dict:
        """Generate safe fallback recommendations when analysis fails"""
        
        elapsed_time = float(query_data.get('elapsed', 0))
        executions = int(query_data.get('executions', 0))
        
        return {
            'sql_id': sql_id,
            'risk_level': 'UNKNOWN',
            'category': 'ERROR_FALLBACK',
            'recommended_indexes': f"Manual analysis required for SQL_ID {sql_id}",
            'query_rewrite': f"Review needed - automated analysis failed", 
            'risk_assessment': f"**ANALYSIS ERROR**\n- SQL_ID: {sql_id}\n- Error: {error_msg}\n- Manual DBA review required",
            'exact_commands': f"Manual investigation needed for SQL_ID {sql_id}",
            'priority': 'Medium',
            'estimated_improvement': 'Manual analysis required'
        }



